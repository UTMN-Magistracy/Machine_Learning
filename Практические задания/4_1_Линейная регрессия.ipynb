{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Линейная регрессия\n\n## Теоретическая часть\n\n## Построение предсказаний\n\n__Вспомнить из лекции:__\n* Как выполняются предсказания с помощью модели линейной регрессии?\n* Как интерпретировать веса в модели линейной регрессии?\n\n### **Задача 1**.\n\nКакие предсказания сделает линейная модель\n\n$$a(x) = w_1 x_1 + w_2 x_2 + w_3 x_3 + b$$\n\nдля объекта $x = (7, -3, 2)$, вектора весов $w = (0.5, 1.5, -2)$ и сдвига $b=10$?\n\n","metadata":{}},{"cell_type":"code","source":"# Код для получения ответа.\n# Пояснения обязательны!\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Задача 2**.\n\nДана матрица $X$ размера $\\ell \\times (d+1)$, $\\ell$ - число объектов, $(d+1)$ - число признаков. В матрицу $X$ входит константный признак, который равен 1 для всех объектов. Также дан вектор весов $w$ длины $(d+1)$. Записать в матричном виде (через матричное произведение), как будут выполняться предсказания для этой матрицы.\n\nУказать размерности всех матриц, входящих в формулу, проверить корректность выполнения матричных операций (с точки зрения размерности).","metadata":{}},{"cell_type":"markdown","source":"**Решение задачи 2:** \n\nтекст решения\n","metadata":{}},{"cell_type":"markdown","source":"### **Задача 3**.\n\nВизуализируйте решающее правило для $d=1$, $w_1=3$, $w_0=-2$. За что отвечает сдвиг $w_0$? Почему исключение сдвига из модели $w_0=0$, скорее всего, ухудшит качество предсказаний?\n","metadata":{}},{"cell_type":"code","source":"# Код, реализующий визуализацию.\n# Пояснения обязательны!\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Вопрос:** \n\nКакой геометрический объект задает решающее правило линейной регрессии в трехмерном случае (два признака)? В многомерном случае (больше двух признаков)?\n","metadata":{}},{"cell_type":"markdown","source":"**Ответ на вопрос:** \n\nтекст ответа\n","metadata":{}},{"cell_type":"markdown","source":"## Обучение линейной регрессии\n__Вспомнить из лекции:__\n* Как выполняется обучение линейной регрессии на основе среднеквадратической ошибки? \n* Какой функционал при этом минимизируется?\n* Как выглядит аналитическое решение задачи обучения (формула для определения весов)?\n* Почему на практике не используется аналитическое решение?\n* Каков оптимизационный алгоритм (как рассчитываются веса)?\n\n### **Задача 4**.\n\nДаны матрица объекты-признаки $X$ размера $\\ell \\times (d+1)$, вектор правильных ответов $Y$ длины $\\ell$ и вектор весов $w$ длины $(d+1)$, $\\ell$ - число объектов, $(d+1)$ - число признаков. Указать все ошибки, допущенные в записи выражения для среднеквадратичной ошибки:\n\n$$\\| Xw - Y \\| = \\frac 1 \\ell \\sum_{j=1}^d \\bigl ( \\sum_{i=1}^\\ell x_{ij} w_{j} - y_j   \\bigr )^2$$\n\nЗаписать правильное выражение.\n","metadata":{}},{"cell_type":"markdown","source":"**Решение задачи 4:** \n\nОшибки:\n1. \n2. \n3. \n\nПравильное выражение:\n","metadata":{}},{"cell_type":"markdown","source":"## Практическая часть\n\nРеализуем класс линейной регрессии, соответствующий интерфейсу **sklearn**, в котором обучение будет реализовано путем применения аналитического метода. \n\nТестирование класса выполним на наборе данных **boston** (хранится в модуле **datasets**), который содержит характеристики и стоимость домов.","metadata":{}},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from sklearn.datasets import load_boston\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  dtype=np.int):\n","output_type":"stream"}]},{"cell_type":"code","source":"# Загрузка набора данных и вывод информации о ключах (характеризуют содержимое набора)\nboston = load_boston()\nboston.keys()","metadata":{"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])"},"metadata":{}}]},{"cell_type":"code","source":"# Формирование матрицы признаков и вектора ответов\nX = boston[\"data\"]\ny = boston[\"target\"]","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n        4.9800e+00],\n       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n        9.1400e+00],\n       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n        4.0300e+00],\n       ...,\n       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n        5.6400e+00],\n       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n        6.4800e+00],\n       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n        7.8800e+00]])"},"metadata":{}}]},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(506, 13)"},"metadata":{}}]},{"cell_type":"markdown","source":"Добавьте единичный столбец в X и сохраните результат в переменную X_with1:","metadata":{}},{"cell_type":"code","source":"### Код, реализующий добавление единичного столбца\n# (506, 13) -> (506, 14)\n","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"X_with1","metadata":{"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 3.9690e+02, 4.9800e+00,\n        1.0000e+00],\n       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 3.9690e+02, 9.1400e+00,\n        1.0000e+00],\n       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 3.9283e+02, 4.0300e+00,\n        1.0000e+00],\n       ...,\n       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 3.9690e+02, 5.6400e+00,\n        1.0000e+00],\n       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 3.9345e+02, 6.4800e+00,\n        1.0000e+00],\n       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 3.9690e+02, 7.8800e+00,\n        1.0000e+00]])"},"metadata":{}}]},{"cell_type":"markdown","source":"Разделим обе выборки на обучение и контроль, зафиксировав random_state, чтобы разбиение в обоих случаях выполнялось одинаково:","metadata":{}},{"cell_type":"code","source":"X_tr, X_te, y_tr, y_te = train_test_split(X, y, random_state=10)\nX_tr_with1, X_te_with1, y_tr, y_te = train_test_split(X_with1, y, random_state=10)","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"X_tr.shape, y_tr.shape, X_tr_with1.shape, X_te_with1.shape","metadata":{"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"((379, 13), (379,), (379, 14), (127, 14))"},"metadata":{}}]},{"cell_type":"markdown","source":"Реализуйте методы **fit** и **predict**. Интерфейс соответствует стандартному интерфейсу **sklearn**. \n\nНапоминание: в машинном обучении классы удобно использовать, чтобы сохранять значения параметров внутри класса после обучения. У класса две основные функции: **fit** (производит обучение по обучающей выборке) и **predict** (выполняет предсказания для заданной выборки). Функция **fit** вызывается один раз, после чего параметры (в данном случае веса) сохраняются внутри класса. После обучения можно много раз вызывать функцию **predict**, чтобы выполнять предсказания на разных выборках. При этом будут использоваться сохраненные внутри класса веса. \n\nВ методе **fit** нужно реализовать вычисление весов по формуле $w = (X^T X)^{-1} X^T Y$ и сохранить в self., в методе **predict** - вернуть предсказания по формуле $X w$ (см. задачи выше), используя сохраненный вектор весов. ","metadata":{}},{"cell_type":"code","source":"class LinearRegression:\n    def __init__(self, learning_method=\"analytical\"):\n        self.learning_method = learning_method\n        \n    def fit(self, X, y):\n        if self.learning_method == \"analytical\":\n            ### код, реализующий вычисление весов\n            \n        else:\n            pass\n            \n    def predict(self, X):\n        ### код, реализующий получение предсказаний\n        \n        return ","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Проверим качество работы полученной модели. Сравним работу модели с константным признаком и без него.","metadata":{}},{"cell_type":"code","source":"regr = LinearRegression(\"analytical\")\nregr.fit(X_tr, y_tr)\nprint(\"train error:\", mean_squared_error(regr.predict(X_tr), y_tr))\nprint(\"test error:\", mean_squared_error(regr.predict(X_te), y_te))","metadata":{"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"train error: 20.696849469975195\ntest error: 35.89242405295997\n","output_type":"stream"}]},{"cell_type":"code","source":"regr = LinearRegression(\"analytical\")\nregr.fit(X_tr_with1, y_tr)\nprint(\"train error:\", mean_squared_error(regr.predict(X_tr_with1), y_tr))\nprint(\"test error:\", mean_squared_error(regr.predict(X_te_with1), y_te))","metadata":{"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"train error: 18.87900085091601\ntest error: 32.44253669601105\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Как и следовало ожидать, качество работы алгоритма с константным признаком лучше, чем без него ($w_0=0$).\n\nКачество работы алгоритма на тестовых данных хуже, чем на обучающей выборке - это стандартная ситуация в машинном обучении.","metadata":{}},{"cell_type":"markdown","source":"### **Дополнительное задание**\n\nРеализовать обучение линейной регрессии с помощью градиентного спуска. В этом случае следует дополнительно учесть следующие моменты.\n* Выборку X необходимо масштабировать (можно использовать sklearn.preprocessing.Normalizer)\n* Длину шага нужно настроить так, чтобы градиентный спуск не расходился (начать с большой, с постепенным уменьшением). Рекомендация: контролировать изменение ошибки в процессе обучения.\n* Может понадобиться выполнить большое число итераций (порядка миллиона).\n\n**Внимание:** все вычисления - ТОЛЬКО В МАТРИЧНОЙ ФОРМЕ. Циклы можно использовать только для организации итераций.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}